{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Module 1: Case Study\"\n",
        "subtitle: \"Bayesian Knowledge Tracing\"\n",
        "author: \"LASER Institute\"\n",
        "date: today \n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    toc-depth: 4\n",
        "    toc-location: right\n",
        "theme:\n",
        "  light: simplex\n",
        "  dark: cyborg\n",
        "editor: visual\n",
        "jupyter: python3\n",
        "bibliography: lit/references.bib\n",
        "---\n",
        "\n",
        "## 1. Prepare\n",
        "\n",
        "The first KT case study is inspired by the work of @zambrano2024investigating, which analyzed the performance of the Bayesian Knowledge Tracing (BKT) model and carelessness detector on every demographic group in the sample. The primary aim of this case study is to gain some hands-on experience with essential Python packages and functions for Bayesian Knowledge Tracing. This case study and those in the modules that follow are organized around data-intensive workflow processes that are common to Learning Analytics [@krumm2018]:\n",
        "\n",
        "![](images/laser-cycle.png){width=\"80%\"}\n",
        "\n",
        "In this case study, you will learn how to do the data wrangling, fit the model, and analyze the goodness of the model. @zambrano2024investigating utilized the brute-force grid search (BKT-BF, in Java) to fit the BKT model but you will use pyBKT(Python) here. pyBKT is easier to start with but slower in performance. Specifically, this case study addresses the follow topics:\n",
        "\n",
        "1.  **Prepare**: Before analysis, you'll read a recent paper about BKT, learn about the current trends, and get introduced to the {pandas}, {sklearn}, and {pyBKT} packages for data wrangling and analyzing the BKT model.\n",
        "\n",
        "2.  **Wrangle**: Then, we will prepare our data for the analysis. This typically involves data cleaning, preprocessing, and transformation to ensure that the data is in a suitable format for modeling.\n",
        "\n",
        "3.  **Explore**: In this part, you will use descriptive statistics. This step helps you understand the characteristics and patterns within the data before proceeding to modeling.\n",
        "\n",
        "4.  **Model**: In the model section of the case study, you will learn basic techniques for fitting and evaluating a BKT model. You will also explore a variant of the BKT model and an advanced feature called Roster in the pyBKT package.\n",
        "\n",
        "5.  **Communication**: This sections will cover how we can share results of your analysis with wider audience.\n",
        "\n",
        "### 1a. Review the Research\n",
        "\n",
        "![](images/case-study-paper.png)\n",
        "\n",
        "[link to the full paper](https://learninganalytics.upenn.edu/ryanbaker/lak24-47-4.pdf)\n",
        "\n",
        "In this study, @zambrano2024investigating assessed the degree to which algorithmic biases are present in two learning analytics models: knowledge estimates based on Bayesian Knowledge Tracing (BKT) and carelessness detectors. Specifically, this analysis evaluated the model performance across demographic groups, compared performance across intersectional groups of these demographics, and explored models' transferability across unobserved demographics. Performance was similar for students of different races, special needs students, English language learners, economically disadvantaged students, and both male and female students. Results show close to equal performance across these groups.\n",
        "\n",
        "#### Research Questions\n",
        "\n",
        "The central goal of this research is to\n",
        "\n",
        "> investigate the degree to which algorithmic biases are present in two learning analytics models: Bayesian Knowledge Tracing (BKT) and carelessness detectors.\n",
        "\n",
        "#### Data Collection\n",
        "\n",
        "The data is from 5,856 students across 12 middle and high schools in a northeastern US city. The students used Carnegie Learning‚Äôs MATHia (@ritter2007cognitive) software for math instruction during the 2021-2022 academic years. The content includes multi-step questions, guiding students through predetermined content sequences. MATHia's structure closely aligns with the Bayesian Knowledge Tracing (BKT) algorithm.\n",
        "\n",
        "#### Analysis\n",
        "\n",
        "The knowledge estimate for specific skills was calculated using BKT. The authors fitted BKT parameters with brute-force grid search. Upper limits of 0.3 and 0.1 for the ‚ÄôGuess‚Äô and ‚ÄôSlip‚Äô parameters were adopted respectively to avoid model degeneracy and ensure the parameter values are aligned with conceptual meaning. Demographic characteristics are not taken into calculation directly when building the BKT model. However, sample sizes are not even across demographic groups so the parameters could be more representative of demographics with a larger number of students.\n",
        "\n",
        "The authors adopted a 4-fold student level cross-validation that was stratified by demographics and evaluated the model performance with AUC ROC. The max difference between AUC for the best and worst predicted group was also calculated.\n",
        "\n",
        "#### Key Findings\n",
        "\n",
        "As reported by @zambrano2024investigating in their findings section:\n",
        "\n",
        "> We found evidence that performance was close to equal across demographic groups, for these models, including intersectional categories, and tests where we held out entire demographic groups during model training (a test of model applicability to entirely new demographic groups), for carelessness.\n",
        "\n",
        "#### ‚ùìQuestion\n",
        "\n",
        "Before we conclude that this model is free of bias, what other tests or data collection might you want to perform?¬†\n",
        "\n",
        "Type a brief response in the space below:\n",
        "\n",
        "### 1b. Load Packages\n",
        "\n",
        "In this case study, you will not replicate the data analysis in @zambrano2024investigating. Instead, you will use the data from @baker2008developing to learn how to fit a BKT model. First, you will learn about the essential packages you will be using in this case study.\n",
        "\n",
        "**Packages**, sometimes called libraries, are shareable collections of Python code that can contain functions, data, and/or documentation and extend the functionality of Python. You can always check to see which Python packages that are not dependencies of other packages have already been installed and loaded into RStudio Cloud using the command `pip list` in the terminal.\n",
        "\n",
        "#### pandas üì¶\n",
        "\n",
        "![](img/pandas.svg){width=\"30%\"}\n",
        "\n",
        "One package that you'll be using extensively is {pandas}. [Pandas](https://pandas.pydata.org) [@mckinney-proc-scipy-2010] is a powerful and flexible open-source data analysis and wrangling tool for Python. Python is also used widely by the data science community.\n",
        "\n",
        "Click the green arrow in the right corner of the \"code chunk\" that follows to load the {pandas} library introduced in LA Workflow labs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Numpyüì¶\n",
        "\n",
        "![](images/numpy.png){width=\"142\"}\n",
        "\n",
        "NumPy (pronounced /Ààn åmpa…™/¬†NUM-py) is a library for the¬†Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
        "\n",
        "Click the green arrow in the right corner of the \"code chunk\" that follows to load **`numpy`**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pyplot üì¶\n",
        "\n",
        "![](img/matplotlib.png){width=\"20%\"}\n",
        "\n",
        "Pyplot is a module in the {matplotlib) package, a comprehensive library for creating static, animated, and interactive visualizations in Python. **`pyplot`** provides a MATLAB-like interface for making plots and is particularly suited for interactive plotting and simple cases of programmatic plot generation.\n",
        "\n",
        "Click the green arrow in the right corner of the \"code chunk\" that follows to load **`pyplot`**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### pyBKT üì¶\n",
        "\n",
        "The {pyBKT} package [@badrinath2021pybkt] is a Python implementation of the Bayesian Knowledge Tracing algorithm and variants, estimating student cognitive mastery from problem-solving sequences [@badrinath2021pybkt].\n",
        "\n",
        "#### **üëâ Your Turn** **‚§µ**\n",
        "\n",
        "Use the code chunk below to import the pyBKT package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Your code starts here\n",
        "from pyBKT.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. WRANGLE\n",
        "\n",
        "Data wrangling is the process of converting raw data into a format suitable for analysis. Typically, you will go through data wrangling before data analysis to ensure the high quality of the data and high-quality data leans to better performance. Thus, data wrangling is of great importance in any analysis.\n",
        "\n",
        "### 2a. Import the dataset\n",
        "\n",
        "To realize the goals, you'll need to first import the CSV files originally obtained from @baker2008developing This data set is a subset of the data set used in @baker2008developing. A description of each file is below along with a link to the original file:\n",
        "\n",
        "1.  [Example CSV dataset](https://learninganalytics.upenn.edu/ryanbaker/EDM2014/AsgnBA3-dataset.zip): This BKT dataset consists of 298 students' performance in 67 skills.\n",
        "\n",
        "2.  [Dataset description](https://learninganalytics.upenn.edu/ryanbaker/EDM2023/basic-asgn-4-CoreMethods-2023-v1.pdf): This file includes the descriptions of all the variables in this dataset.\n",
        "\n",
        "Let's use the `read_csv` function from the {pandas} package to import the `AsgnBA3-dataset.csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.read_csv(\"data/AsgnBA3-dataset.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`df` here stands for \"DataFrame\" in the Pandas package. A Pandas DataFrame is a two-dimensional data structure, like a two-dimensional array, or a table with rows and columns.\n",
        "\n",
        "**`head()`** on a DataFrame object will display the first 5 rows of the DataFrame.\n",
        "\n",
        "#### Remove unnecessary rows\n",
        "\n",
        "In BKT, only the first attempts matter and are included in calculation. Before moving to the next step, use the code chunk below to filter out all the rows in which the firstattempt equals 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df2 = df[df[\"firstattempt\"]==1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just as a refresher, in the code above we created a new DataFrame **`df2`** by filtering the original DataFrame **`df`**. In the expression **`df[df[\"firstattempt\"]==1]`**, the brackets serve a specific function in the context of Python's Pandas library:\n",
        "\n",
        "1.  **Outer Brackets**: These are used to access elements of the DataFrame **`df`**. In Pandas, brackets are used to select columns (when you use a string or list) or rows (when you use slicing or boolean indexing).\n",
        "\n",
        "2.  **Inner Brackets**: The inner brackets contain the condition **`df[\"firstattempt\"]==1`**. Here, **`df[\"firstattempt\"]`** selects the column named \"firstattempt\" from the DataFrame, and selects only the rows where the value of the column **`\"firstattempt\"`** is equal to 1.\n",
        "\n",
        "Essentially, **`df2`** will contain all the rows from **`df`** where the first attempt at a the test question is marked by a 1, indicating that it was indeed the student's first attempt.\n",
        "\n",
        "#### **üëâ Your Turn** **‚§µ**\n",
        "\n",
        "Now filter out all the rows that represent the \"CHOOSE-X-AXIS-QUANTITATIVE\" skill and assign the result to the variable `df3` ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df3 = df2[df2[\"KC\"]==\"CHOOSE-X-AXIS-QUANTITATIVE\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mapping the column names\n",
        "\n",
        "The accepted input formats in pyBKT are Pandas DataFrames and data files of type CSV (comma separated) or TSV (tab separated). pyBKT will automatically infer which delimiter to use in the case that it is passed a data file. Since column names mapping meaning to each field in the data (i.e. skill name, correct/incorrect) varies per data source, you may need to specify a mapping from your data file's column names to pyBKT's expected column names.\n",
        "\n",
        "Thus, you will need to create a column name mapping before training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "defaults = {'order_id': 'ID', 'skill_name': 'KC', 'correct': 'right', 'user_id': 'Student'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a dictionary. Dictionaries are used to store data values in key: value pairs. Dictionaries are written with curly brackets. Here‚Äôs what each part of the code is doing:\n",
        "\n",
        "-   **`defaults = {...}`**: This initializes **`defaults`** as a Python dictionary. Dictionaries in Python are collections of key-value pairs, which are used to store data values like a map.\n",
        "\n",
        "-   **Key-Value Pairs**: Inside the dictionary, there are several key-value pairs defined:\n",
        "\n",
        "    -   **`'order_id': 'ID'`** maps the key **`'order_id'`** to the value **`'ID'`**.\n",
        "\n",
        "    -   **`'skill_name': 'KC'`** maps the key **`'skill_name'`** to the value **`'KC'`**.\n",
        "\n",
        "    -   **`'correct': 'right'`** maps the key **`'correct'`** to the value **`'right'`**.\n",
        "\n",
        "    -   **`'user_id': 'Student'`** maps the key **`'user_id'`** to the value **`'Student'`**.\n",
        "\n",
        "The column names you need to specify are order_id, skill_name, correct, and user_id. You may refer to the document in 2a about the descriptions of all the variables.\n",
        "\n",
        "## 3. EXPLORE\n",
        "\n",
        "Before we begin learning about how to fit a basic BKT model, we'll use several methods to explore our data.\n",
        "\n",
        "### 3a. Descriptive statistics\n",
        "\n",
        "### Head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**`head()`** is a function provided by pandas for DataFrames. When you call **`head()`** on a DataFrame, it shows you the first few rows of your data. By default, it displays the first 5 rows, but you can specify a different number if you want.\n",
        "\n",
        "#### Describe()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df3.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  **describe()**: **`describe()`** is a function provided by pandas for DataFrames. When you call **`describe()`** on a DataFrame like **`df3`**, it gives you some basic statistics about the data in each column. These statistics include things like count, mean, standard deviation, minimum, maximum, and various percentiles.\n",
        "\n",
        "#### Histogram of Time Column\n",
        "\n",
        "We could also create a Histogram of the time column using the code chunk below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df3.hist(column='time', bins= 10)\n",
        "plt.xlabel('Response Time')\n",
        "plt.ylabel('Number')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  **`import matplotlib.pyplot as plt`**: This line imports the Matplotlib library, specifically the submodule **`pyplot`**, and assigns it the alias **`plt`**. This makes it easier to refer to Matplotlib functions later in the code.\n",
        "\n",
        "2.  **`hist_time = df3.hist(column='time')`**: This line creates a histogram of a specific column named 'time' from the DataFrame called **`df3`**. In this line:\n",
        "\n",
        "    -   **`df3`** is the DataFrame we have cleaned so far.\n",
        "\n",
        "    -   **`hist()`** is a function provided by pandas to create histograms. You're specifying the column you want to create the histogram for by passing the argument **`column='time'`**.\n",
        "\n",
        "    -   The result of **`hist()`** is stored in the variable **`hist_time`**, although it's not necessary for the code to function. This variable could be used later if you want to modify or analyze the histogram further.\n",
        "\n",
        "3.  **`plt.show()`**: This line displays the histogram that was created. After you've created a plot using Matplotlib, you typically use **`plt.show()`** to actually visualize it. Without this line, the histogram would be created but not shown on the screen.\n",
        "\n",
        "If you would like to look into a specific range, you could add the parameter `range=` . For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df3.hist(column='time', bins= 10, range=[0,20])\n",
        "plt.xlabel('Response Time')\n",
        "plt.ylabel('Number')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A parameter refers to a variable that is used in a function or method to receive a value when the function is called. Here's a simple example in Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def greet(name):\n",
        "    print(\"Hello, \" + name + \"!\")\n",
        "\n",
        "greet(\"Alice\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, **`name`** is a parameter of the **`greet`** function. When we call the **`greet`** function and pass in the argument \"Alice\", the value of \"Alice\" is assigned to the **`name`** parameter within the function.\n",
        "\n",
        "Parameters allow functions to be more flexible and reusable because they enable the function to work with different data each time it is called.\n",
        "\n",
        "## 4. MODEL\n",
        "\n",
        "In this section we'll learn to fit a basic BKT model, but what exactly does is mean to \"fit\" a model?\n",
        "\n",
        "This is an example of model fitting:\n",
        "\n",
        "![](images/fitting.png){width=\"370\"}\n",
        "\n",
        "In this example, linear regression is used to fit the model and this is one of the simplest techniques. You are fitting the model (the line) to a dataset (the dots). The model will be on the form y = a x + b, and you‚Äôre trying to find the optimal values of a and b. You draw a line that best fits the existing data points on average. Once you‚Äôve fitted the model, you can use it to predict outcomes (y-axis) based on inputs (x-axis).\n",
        "\n",
        "### 4a. Fit the model\n",
        "\n",
        "pyBKT makes fitting the model very easy. It only takes 2 lines:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = Model(seed = 42)\n",
        "model.fit(data = df3, defaults = defaults)\n",
        "print(model.params())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, use the `Model` function of the pyBKT package to create a BKT model.\n",
        "\n",
        "The `seed` parameter is used to initialize the random number generator. The random number generator needs a number to start with (a seed value), to be able to generate a random number.\n",
        "\n",
        "Then, use the `fit` method and input `df3` and `defaults`, the dataset you have cleaned so far, and the column name mapping.\n",
        "\n",
        "Let's run the code chunk below to see what are the best parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.params())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### RMSE and AUC\n",
        "\n",
        "pyBKT provides various ways to evaluate your BKT model, such as RMSE, and AUC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "training_rmse = model.evaluate(data = df3) \n",
        "training_auc = model.evaluate(data= df3, metric = 'auc') \n",
        "\n",
        "print(\"Training RMSE: %f\" % training_rmse) \n",
        "print(\"Training AUC: %f\" % training_auc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Root Mean Squared Error (RMSE) is¬†one of the two main performance indicators for a regression model. It measures the average difference between values predicted by a model and the actual values. To put it simply, the lower, the better.\n",
        "\n",
        "AUC ROC is the area under the ROC curve. [@baker2024fixingEach] point out that the ROC curve shows the trade-off between the sensitivity and specificity of the model. When expanded to the entire curve, it shows this trade-off across all possible thresholds.\n",
        "\n",
        "#### Create additional metrics\n",
        "\n",
        "You can even create metrics, such as the sum of squared residuals (SSR), used by BKT-BF @baker2010contextual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def SSR(true_vals, pred_vals):   \n",
        "  return np.sum(np.square(true_vals - pred_vals))  \n",
        "training_SSR = model.evaluate(data= df3, metric = SSR) \n",
        "print(\"Training SSR: %f\" % training_SSR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sum of squared residuals (SSR) **measures the level of variance in the error term, or residuals, of a regression model**. The smaller the residual sum of squares, the better your model fits your data; the greater the residual sum of squares, the poorer your model fits your data.\n",
        "\n",
        "#### **üëâ Your Turn** **‚§µ**\n",
        "\n",
        "Split the data into the training set and the testing set by 80%/20% and fit your BKT model on a skill. This skill should not be the same as the one above or the skill in the ASSISTments activity. Then, evaluate the RMSE and AUC on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#split the dataset by 80%/20%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4b. Conditionalizing in BKT\n",
        "\n",
        "#### Conditionalize guess, slip, and learn on other factors\n",
        "\n",
        "You can also conditionalize guess, slip, or learn on other factors in the BKT model -- something used for practical purposes [@beck2008does] and [@baker2018modeling]. You need to provide guess/slip/learn classes to use in fitting the model. Let's say you are going to fit each item's priors, learn rate, guess, and slip model with the data @pardos2011kt. You will need to first specify which column is the class.\n",
        "\n",
        "Use the code chunk below to test this variant:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "defaults_multi = {'order_id': 'ID', 'skill_name': 'KC', 'correct': 'right', 'user_id': 'Student', 'multigs': 'item', 'multilearn':'item'}\n",
        "model_multi = Model(seed=42, num_fits = 1)\n",
        "\n",
        "model_multi.fit(data= df3, multilearn = True, multigs= True, defaults = defaults_multi)\n",
        "print(model_multi.params())  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see from the output, each item now has the guess, slip, and learn rate. You can conditionalize on lots of factors, depending on the dataset you collected.\n",
        "\n",
        "#### **üëâ Your Turn** **‚§µ**\n",
        "\n",
        "### ‚ùìChallenge: Conditionalize slip on response time\n",
        "\n",
        "For example, perhaps you want to modify to conditionalize slip on whether the time taken was under 5 seconds. Thus, you will have two sets of the 4 classical parameters: one set is for those actions that took more than 5 seconds and the other set is for those actions that take less than 5 seconds. Please notice that this is not exactly great practice. A more sophisticated approach is seen in @baker2008more but it‚Äôs feasible with the data set you have.\n",
        "\n",
        "The current dataset only has response time. You will need to create another binary column for this challenge. Please build this model on the \"VALUING-CAT-FEATURES\" skill.\n",
        "\n",
        "#### Build a Column on whether the students took more than 5 seconds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#This part is just an example and the code will not show to the participants  \n",
        "df4= df2[df2[\"KC\"]==\"VALUING-CAT-FEATURES\"]\n",
        "df4.insert(8, 'FiveSecs', 0)  \n",
        "def cal_5secs(row):    \n",
        "  if row['time'] > 5:         \n",
        "    return 1    \n",
        "  else:         \n",
        "    return 0  \n",
        "df4[\"FiveSecs\"] = df4.apply(cal_5secs, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Build your 5 seconds model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#This part is just an example and the code will not show to the participants  \n",
        "defaults_5sec = {'order_id': 'ID', 'skill_name': 'KC', 'correct': 'right', 'user_id': 'Student', 'multigs': 'FiveSecs'}  \n",
        "model_5sec = Model(seed = 42, num_fits = 1) \n",
        "model_5sec.fit(data= df4, multigs = True,defaults = defaults_5sec) \n",
        "print(model_5sec.params())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4c. Cross Validation in BKT\n",
        "\n",
        "pyBKT also allows you to easily use cross-validation.\n",
        "\n",
        "Cross-validation is a technique to evaluate the performance of a model on unseen data. The picture below shows how the 5-fold cross-validation works.\n",
        "\n",
        "![](images/cross-validation.png){width=\"80%\"}\n",
        "\n",
        "Cross-validation is offered as function similar to a combination of fit and evaluate that accepts a particular number of folds, a seed, and a metric(either one of the 3 provided 'rmse', 'auc' or 'accuracy' -- a custom Python function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model2 = Model(seed = 42, num_fits=1)\n",
        "cross_vali = model2.crossvalidate(data = df3, folds = 10, defaults = defaults, metric = 'auc')\n",
        "print(cross_vali)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **üëâ Your Turn** **‚§µ**\n",
        "\n",
        "### ‚ùìChallenge: Cross-validation on your response time model\n",
        "\n",
        "Please use the code chunk below to conduct 10-fold cross-validation on your response time model and answer the following question:\n",
        "\n",
        "Is the model better than classic BKT when conditionalizing slip on response time? Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# It is just an example. It will not show in students' version\n",
        "crossvali_5sec = Model(seed = 42)\n",
        "cross_validation = crossvali_5sec .crossvalidate(data = df4, folds = 10, defaults = defaults_5sec, metric = 'auc', multigs= True)\n",
        "print(cross_validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4c. Other advanced features in pyBKT\n",
        "\n",
        "pyBKT also offers some other advanced features, such as Roster and Parameter Fixing.\n",
        "\n",
        "### 4a. Roster\n",
        "\n",
        "Roster is used to simulate the learning environment for a group of students learning any combination of individual skills.\n",
        "\n",
        "You need to first create a backend pyBKT model and fit it on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "defaults_roster = {'order_id': 'ID', 'skill_name': 'KC', 'correct': 'right', 'user_id': 'Student'}\n",
        "model_roster = Model()\n",
        "model_roster.fit(data = df2, defaults = defaults_roster )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then you can use the `Roster` to create a roster with two students and one skill."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyBKT.models import *\n",
        "roster = Roster(students = ['Jack', 'Rachel'], skills = \"VALUING-NUM-FEATURES\", model = model_roster)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can update Rachel's status by adding one or more responses to a particular skill. In this case, Rachel correctly answered one question. Then check Rachel's updated mastery state and probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rachel_new_state = roster.update_state('VALUING-NUM-FEATURES', 'Rachel', 1)\n",
        "print(\"Rachel's mastery:\", roster.get_state_type('VALUING-NUM-FEATURES', 'Rachel'))\n",
        "print(\"Rachel's probability of mastery:\", roster.get_mastery_prob('VALUING-NUM-FEATURES', 'Rachel'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **üëâ Your Turn** **‚§µ**\n",
        "\n",
        "Create a new roster on the model you fitted in section 2. If you add 3 consecutive correct answers to Rachel and 3 consecutive incorrect answers to Jack, will they be assessed as having mastered the skill or not? Use the code chunk below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Add 3 consecutive correct answers to Rachel and 3 consecutive incorrect answers to Jack,"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4b. Parameter Fixing\n",
        "\n",
        "Another advanced feature supported by pyBKT is parameter fixing, where you can fix one or more parameters and train the model conditioned on those fixed parameters. For example, you could fix the slip rate to 0.2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_fixedparam = Model()\n",
        "defaults = {'order_id': 'ID', 'skill_name': 'KC', 'correct': 'right', 'user_id': 'Student'}\n",
        "model_fixedparam.coef_ = {'CHOOSE-X-AXIS-QUANTITATIVE': {'slips': np.array([0.2])}}\n",
        "model_fixedparam.fit(data = df3, fixed=True, defaults = defaults)\n",
        "model_fixedparam.params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **üëâ Your Turn** **‚§µ**\n",
        "\n",
        "Fix the slip rate to 0.3 and the guess rate to 0.2 when fitting your model, Does the model perform better or worse?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. COMMUNICATE\n",
        "\n",
        "### 5a Visualization\n",
        "\n",
        "#### params()\n",
        "\n",
        "As you may have noticed above, it is very easy to visualize the parameters of your fitted model. Use the code chunk below to check the parameters of the model that conditionalizes on items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_multi.params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**`model.params()`** function is used to retrieve the parameters after you fit the model.\n",
        "\n",
        "#### Pyplot\n",
        "\n",
        "The pyplot, a module of the matplotlib, is another powerful tool to visualize your data.\n",
        "\n",
        "Use the code chunk below to compare the AUC ROC of the classic BKT model, the BKT model that conditionalizes on response time, and the BKT model that conditionalizes on items when conducting 5-fold cross-validation on the \"VALUING-NUM-FEATURES\" skill.\n",
        "\n",
        "Run the code chunk below to cross-validate three models respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Example code\n",
        "df_compare = df2[df2[\"KC\"]==\"QUANTITATIVE-VALUING-DETERMINE-ARBITRARY-SCALEKNOWN\"]\n",
        "print(df_compare)\n",
        "model_5 = Model()\n",
        "model_item = Model()\n",
        "model_classic = Model()\n",
        "if \"FiveSecs\" in df_compare.columns:\n",
        "  print(\"Already there!\")\n",
        "else:\n",
        "  df_compare.insert(8, 'FiveSecs', 0)\n",
        "df_compare[\"FiveSecs\"] = df_compare.apply(cal_5secs, axis=1)\n",
        "\n",
        "types = {}\n",
        "types['classic'] = model_classic.crossvalidate(data = df_compare, folds = 5, defaults = defaults, metric = 'auc')\n",
        "types['five'] = model_5.crossvalidate(data = df_compare, folds = 5, defaults = defaults_5sec, metric = 'auc', multigs = True)\n",
        "types['item'] = model_item.crossvalidate(data = df_compare, folds = 5, defaults = defaults_multi, metric = 'auc', multigs = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then use the code chunk below to plot them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_auc = pd.concat(types.values())\n",
        "print(df_auc)\n",
        "df_auc['modeltype'] = types.keys()\n",
        "plt.figure(figsize = (6, 4))\n",
        "plt.plot(df_auc['modeltype'], df_auc['auc'])\n",
        "plt.title('AUC of Model Variants')\n",
        "plt.ylabel('AUC')\n",
        "plt.xlabel('Model Variants')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "let's break down this code step by step:\n",
        "\n",
        "1.  **`df_auc = pd.concat(types.values())`**: This line creates a DataFrame called **`df_auc`** by concatenating the values of a dictionary called **`types`**. Each value in the dictionary is assumed to be a DataFrame. This concatenation essentially stacks these DataFrames on top of each other to create a single DataFrame.\n",
        "\n",
        "2.  **`print(df_auc)`**: This line prints out the DataFrame **`df_auc`**, showing its contents. This is useful for inspecting the data and understanding its structure.\n",
        "\n",
        "3.  **`df_auc['modeltype'] = types.keys()`**: This line adds a new column to the DataFrame **`df_auc`** called 'modeltype'. It assigns the keys of the **`types`** dictionary to this column. This associates each row in the DataFrame with a particular model type.\n",
        "\n",
        "4.  **`plt.figure(figsize=(6, 4))`**: This line creates a new figure for plotting with Matplotlib and specifies its size to be 6 inches wide and 4 inches tall. This sets up the canvas for our plot.\n",
        "\n",
        "5.  **`print(df_auc['auc'])`**: This line prints out the 'auc' column of the DataFrame **`df_auc`**. This column presumably contains the Area Under the Curve (AUC) values, which are commonly used to evaluate the performance of classification models.\n",
        "\n",
        "6.  **`plt.plot(df_auc['modeltype'], df_auc['auc'])`**: This line creates a line plot using Matplotlib. It specifies the x-values to be the 'modeltype' column of **`df_auc`** and the y-values to be the 'auc' column. This will plot the AUC values for each model variant against their respective model types.\n",
        "\n",
        "7.  **`plt.title('AUC of Model Variants')`**: This line sets the title of the plot to 'AUC of Model Variants'.\n",
        "\n",
        "8.  **`plt.ylabel('AUC')`**: This line sets the label for the y-axis to 'AUC', indicating that the y-values represent Area Under the Curve.\n",
        "\n",
        "9.  **`plt.xlabel('Model Variants')`**: This line sets the label for the x-axis to 'Model Variants', indicating the different variants of models being compared.\n",
        "\n",
        "10. **`plt.show()`**: This line displays the plot that was created using Matplotlib. After you've set up your plot, you use **`plt.show()`** to actually visualize it on the screen.\n",
        "\n",
        "#### **üëâ Your Turn** **‚§µ**\n",
        "\n",
        "Use the code chunk below to plot the slips and guesses for each class of your response time model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore')\n",
        "params = model_5sec.params()\n",
        "plt.figure(figsize = (6, 4))\n",
        "plt.plot(params.loc[(\"VALUING-CAT-FEATURES\", 'guesses')], label = 'Guesses')\n",
        "plt.plot(params.loc[(\"VALUING-CAT-FEATURES\", 'slips')], label = 'Slips')\n",
        "plt.xlabel('Response Time')\n",
        "plt.ylabel('Rate')\n",
        "plt.title('BKT Parameters per Template ID Class')\n",
        "plt.legend();\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5b Render and Publish your file\n",
        "\n",
        "#### Render File\n",
        "\n",
        "For your SNA Badge, you will have an opportunity to create a simple \"data product\" designed to illustrate some insights gained from your analysis and ideally highlight an action step or change idea that can be used to improve learning or the contexts in which learning occurs.\n",
        "\n",
        "For now, we will wrap up this case study by converting your work to an HTML file that can be published and used to communicate your learning and demonstrate some of your new R skills. To do so, you will need to \"render\" your document by clicking the ![](images/Render.png)\n",
        "\n",
        "Render button in the menu bar at that the top of this file.\n",
        "\n",
        "Rendering a document does two important things:\n",
        "\n",
        "1.  checks through all your code for any errors; and,\n",
        "\n",
        "2.  creates a file in your directory that you can use to share you work .\n",
        "\n",
        "Now that you've finished your first case study, click the \"Render\" button in the toolbar at the top of your document to covert this Quarto document to a HTML web page, just one of [the many publishing formats you can create with Quarto](https://quarto.org/docs/output-formats/all-formats.html) documents.\n",
        "\n",
        "If the files rendered correctly, you should now see a new file named `kt-1-case-study.html` in the Files tab located in the bottom right corner of R Studio. If so, congratulations, you just completed the getting started activity! You're now ready for the unit Case Studies that we will complete during the third week of each unit.\n",
        "\n",
        "::: callout-important\n",
        "If you encounter errors when you try to render, first check the case study answer key located in the files pane and has the suggested code for the Your Turns. If you are still having difficulties, try copying and pasting the error into Google or ChatGPT to see if you can resolve the issue. Finally, contact your instructor to debug the code together if you're still having issues.\n",
        ":::\n",
        "\n",
        "#### Publish File\n",
        "\n",
        "Rendered HTML files can be published online through a variety of ways including [Posit Cloud](https://posit.cloud/learn/guide#publish-from-cloud), [RPubs](#0) , [GitHub Pages](#0), [Quarto Pub](#0), or [other methods](#0). The easiest way to quickly publish your file online is to publish directly from RStudio.\n",
        "\n",
        "![](images/publish.png)\n",
        "\n",
        "Congratulations, you've completed the case study!\n",
        "\n",
        "## References"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/cloud/python/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}